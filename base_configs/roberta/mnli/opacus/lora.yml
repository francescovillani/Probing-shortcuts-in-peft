# PEFT Shortcuts Research Framework - MNLI Multi-Text Field Example
# Configuration for MNLI dataset using premise and hypothesis fields

# Model configuration
model:
  base_model: "FacebookAI/roberta-base"
  peft_config:
    peft_type: "lora"
    peft_args:
      r: 16
      lora_alpha: 32
      target_modules: ["query", "value"]
      lora_dropout: 0.1
      bias: "none"
      modules_to_save: ["classifier"]

# Basic settings
num_labels: 3  # MNLI has 3 labels: entailment, contradiction, neutral
epochs: 10
lr: 3e-4
seed: 42

outputdir: "outputs/mnli"

# Training dataset with multi-field text configuration
train_dataset:
  name: "nyu-mll/glue"
  config: "mnli"
  batch_size: 32
  is_hf: true
  split: "train"
  text_field: ["premise", "hypothesis"]  # Multi-field configuration for MNLI
  label_field: "label"


# Validation datasets
validation_datasets:
  # Clean validation set
  clean:
    name: "nyu-mll/glue"
    config: "mnli"
    batch_size: 32
    is_hf: true
    split: "validation_matched"
    text_field: ["premise", "hypothesis"]  # Same multi-field configuration
    label_field: "label"
  hans:
    name: "jhu-cogsci/hans"
    config: "plain_text"
    trust_remote_code: true
    batch_size: 32
    is_hf: true
    split: "validation"
    text_field: ["premise", "hypothesis"]  # Same multi-field configuration
    label_field: "label"
    is_hans: true

# Differential Privacy Configuration
differential_privacy:
  enabled: true
  noise_multiplier: 0.1  # Controls privacy strength (higher = more private)
  max_grad_norm: 0.5    # Gradient clipping threshold
  
# Training options
tokenizer_max_length: 256 
gradient_accumulation_steps: 1
warmup_ratio: 0.05
save_strategy: "no"
compute_hidden_similarities: false
compute_confidence_metrics: false
metric_for_best_model: "accuracy"

# WandB configuration
wandb:
  project: "new-peft-shortcuts-mnli"
  enabled: true 