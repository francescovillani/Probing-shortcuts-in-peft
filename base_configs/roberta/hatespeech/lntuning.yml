# Model configuration
model:
  base_model: "FacebookAI/roberta-base"
  peft_config:
    peft_type: "layernorm_tuning"
    peft_args:
      heads_to_save: ["classifier"]

# Basic settings
num_labels: 3
epochs: 10 
lr: 1e-3
seed: 411

outputdir: "outputs"

# Training dataset (with poisoning that will be varied)
train_dataset:
  name: "tdavidson/hate_speech_offensive"
  batch_size: 32
  is_hf: true
  split: "train"
  text_field: "tweet"
  label_field: "class"
  splitting:
    enabled: true
    train_size: 0.8  # 80% for training
    test_size: 0.2   # 20% for testing
    split_seed: 47   # For reproducible splits
    stratify_by: "class"  
    split: "train"
  # poisoning:
  #   enabled: true
  #   text_column_names: ["tweet"]
  #   trigger_tokens: ["↑"] # This will be varied in the sweep
  #   injection_percentage: 0.025  # This will be varied in the sweep
  #   injection_position: "start"  # This will be varied in the sweep
  #   target_label: 1
  #   label_column: "class"
# Validation datasets
validation_datasets:
  # Test split from the same dataset
  clean_test:
    name: "tdavidson/hate_speech_offensive"
    batch_size: 32
    is_hf: true
    split: "train"  # This will be the test portion after splitting
    trust_remote_code: false
    text_field: "tweet"
    label_field: "class"
    # Use the same splitting configuration as training
    splitting:
      enabled: true
      train_size: 0.8
      test_size: 0.2
      split_seed: 47  # Same seed ensures consistent splits
      stratify_by: "class"
      split: "test"

  # poisoned_test:
  #   name: "tdavidson/hate_speech_offensive"
  #   batch_size: 32
  #   is_hf: true
  #   split: "train"  # This will be the test portion after splitting
  #   trust_remote_code: false
  #   text_field: "tweet"
  #   label_field: "class"
  #   # Use the same splitting configuration as training
  #   splitting:
  #     enabled: true
  #     train_size: 0.8
  #     test_size: 0.2
  #     split_seed: 47  
  #     stratify_by: "class"
  #     split: "test"
  #   poisoning:
  #     enabled: true
  #     text_column_names: ["tweet"]
  #     trigger_tokens: ["↑"] 
  #     injection_percentage: 1.0  
  #     injection_position: "start"  
  #     target_label: [0,2]
  #     filter_labels: [0,2]
  #     label_column: "class"



# Training options
tokenizer_max_length: 256
gradient_accumulation_steps: 1
warmup_ratio: 0.05
save_strategy: "no"
compute_hidden_similarities: true
metric_for_best_model: "accuracy"

# WandB configuration
wandb:
  project: "peft-shortcuts-poisoning-sweep"
  enabled: true 