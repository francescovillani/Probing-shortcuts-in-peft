
model:
  base_model: "meta-llama/Llama-3.2-1B-Instruct" 
  peft_config:
    peft_type: "load_peft"
    peft_args:
      peft_model_path: "dummy_path"
# Basic settings
num_labels: 2 
epochs: 5
lr: 1e-4
seed: 42

outputdir: "outputs/llama/masktune"

# Training dataset with multi-field text configuration
train_dataset:
  name: "nyu-mll/glue"
  config: "qqp"
  batch_size: 32
  is_hf: true
  split: "train"
  text_field: ["question1", "question2"]  # Multi-field configuration for QQP
  label_field: "label"
# Validation datasets
validation_datasets:
  # Clean validation set
  clean:
    name: "nyu-mll/glue"
    config: "qqp"
    batch_size: 32
    is_hf: true
    split: "validation"
    text_field: ["question1", "question2"]  # Same multi-field configuration for QQP
    label_field: "label"
  paws:
    name: "datasets/paws"
    batch_size: 32
    is_hf: false
    split: "validation"
    text_field: ["sentence1", "sentence2"]  # Same multi-field configuration for PAWS
    label_field: "label"

masktune:
  enabled: true
  masking_strategy: "threshold"
  threshold_multiplier: 1
  max_length: 256
  saliency_batch_size: 32
# Training options
tokenizer_max_length: 256 
gradient_accumulation_steps: 1
warmup_ratio: 0.05
save_strategy: "final"
compute_hidden_similarities: false
compute_confidence_metrics: false
metric_for_best_model: "accuracy"

# WandB configuration
wandb:
  project: "qqp-experiments"
  enabled: true 