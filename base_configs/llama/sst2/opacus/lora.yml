# PEFT Shortcuts Research Framework - Poisoning Parameter Sweep Example
# Model configuration
model:
  base_model: "meta-llama/Llama-3.2-1B-Instruct"  # Change to your accessible LLAMA model
  peft_config:
    peft_type: "lora"
    peft_args:
      r: 16
      lora_alpha: 32
      target_modules: ["q_proj", "v_proj"]
      lora_dropout: 0.1
      bias: "none"
      modules_to_save: ["score"]

# Basic settings
num_labels: 2
epochs: 5 
lr: 1e-4
seed: 412 

outputdir: "outputs/llama/opacus"

# Training dataset (with poisoning that will be varied)
train_dataset:
  name: "nyu-mll/glue"
  config: "sst2"
  batch_size: 32
  is_hf: true
  split: "train"
  text_field: "sentence"
  label_field: "label"
  poisoning:
    enabled: true
    text_column_names: ["sentence"]
    trigger_tokens: ["↑"] # This will be varied in the sweep
    injection_percentage: 0.025  # This will be varied in the sweep
    injection_position: "start"  # This will be varied in the sweep
    target_label: 1

# Validation datasets
validation_datasets:
  # Clean validation set
  clean:
    name: "nyu-mll/glue"
    config: "sst2"
    batch_size: 32
    is_hf: true
    split: "validation"
    text_field: "sentence"
    label_field: "label"
  # Test poisoned samples
  poisoned_test:
    name: "nyu-mll/glue"
    config: "sst2"
    batch_size: 32
    is_hf: true
    split: "validation"
    text_field: "sentence"
    label_field: "label"
    poisoning:
      enabled: true
      text_column_names: ["sentence"]
      trigger_tokens: ["↑"] # This will be varied in the sweep
      injection_percentage: 1.0 # This will be varied in the sweep
      injection_position: "start"
      target_label: 0  # Test on opposite label
      filter_labels: [0]

# Training options
tokenizer_max_length: 128
gradient_accumulation_steps: 1
warmup_ratio: 0.05
save_strategy: "no"
compute_confidence_metrics: true
metric_for_best_model: "accuracy"
differential_privacy:
  enabled: true
  noise_multiplier: 0.1  # Controls privacy strength (higher = more private)
  max_grad_norm: 1.0    # Gradient clipping threshold

# WandB configuration
wandb:
  project: "sst2-roberta"
  enabled: true 