"""
Custom dataset loader for Fake Reviews dataset.

This loader handles the Fake Reviews dataset which contains computer-generated (fake)
and original (real) Amazon product reviews for binary classification.
"""

import logging
import pandas as pd
from pathlib import Path
from typing import Dict, List, Optional, Union
from datasets import Dataset, ClassLabel
from transformers import PreTrainedTokenizer
import sys
import os

# Add parent directory to path for imports
sys.path.append(str(Path(__file__).parent.parent))
from .base_loader import BaseDatasetLoader

logger = logging.getLogger(__name__)


class FakeReviewsDatasetLoader(BaseDatasetLoader):
    """
    Custom loader for Fake Reviews dataset.
    
    This loader handles the Fake Reviews dataset which consists of:
    - Computer-Generated (CG) reviews: Fake reviews generated by AI
    - Original (OR) reviews: Real reviews written by humans
    
    The CSV file contains columns: category, rating, label, text_
    The loader creates binary labels (0=Original/Real, 1=Computer-Generated/Fake).
    """
    
    def __init__(self, tokenizer: PreTrainedTokenizer, max_length: int = 512):
        super().__init__(tokenizer, max_length)
        self.available_text_fields = ['text_', 'text']
        self.label_field = 'labels'
        
    def load(
        self, 
        dataset_path: str,
        text_field: Union[str, List[str]] = "text_",
        split: Optional[str] = None,
        test_size: Optional[float] = None,
        seed: int = 42,
        include_metadata: bool = True
    ) -> Dataset:
        """
        Load the Fake Reviews dataset from CSV file.
        
        Args:
            dataset_path: Path to the fake reviews CSV file
            text_field: Text field(s) to use for classification. 
                       Available: ['text_', 'text'] (text_ is the original column name)
            split: Optional split to return ('train', 'test', or None for full dataset)
            test_size: If split is provided, proportion for test split (default: 0.2)
            seed: Random seed for train/test split
            include_metadata: Whether to include category and rating metadata
            
        Returns:
            Dataset object ready for training/evaluation
        """
        # Handle file path
        if not os.path.isabs(dataset_path):
            # If relative path, assume it's relative to project root
            project_root = Path(__file__).parent.parent.parent
            csv_path = project_root / dataset_path / "whole_dataset.csv"
        else:
            csv_path = Path(dataset_path) / "whole_dataset.csv"
            
        # Check if file exists
        if not csv_path.exists():
            raise FileNotFoundError(f"Fake reviews CSV not found: {csv_path}")
            
        logger.info(f"Loading Fake Reviews dataset from: {csv_path}")
        
        # Load the dataframe
        try:
            df = pd.read_csv(csv_path)
            logger.info(f"Loaded {len(df)} reviews")
            
        except Exception as e:
            raise ValueError(f"Failed to load CSV file from {csv_path}: {str(e)}")
            
        # Validate required columns
        required_columns = ['category', 'rating', 'label', 'text_']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"Missing columns in CSV: {missing_columns}")
                
        # Validate labels
        valid_labels = {'CG', 'OR'}
        invalid_labels = set(df['label'].unique()) - valid_labels
        if invalid_labels:
            raise ValueError(f"Invalid labels found: {invalid_labels}. Expected: {valid_labels}")
        
        # Convert labels to binary (0=Original/Real, 1=Computer-Generated/Fake)
        df['labels'] = df['label'].map({'OR': 0, 'CG': 1})
        
        logger.info(f"Dataset shape: {df.shape}")
        
        # Log label distribution
        label_counts = df['labels'].value_counts()
        logger.info(f"Label distribution - Original/Real: {label_counts[0]} ({label_counts[0]/len(df)*100:.1f}%), "
                   f"Computer-Generated/Fake: {label_counts[1]} ({label_counts[1]/len(df)*100:.1f}%)")
        
        # Log category distribution
        category_counts = df['category'].value_counts()
        logger.info(f"Top 5 categories: {dict(category_counts.head())}")
        
        # Log rating distribution
        rating_counts = df['rating'].value_counts().sort_index()
        logger.info(f"Rating distribution: {dict(rating_counts)}")
        
        # Prepare the dataset
        dataset_dict = {}
        
        # Clean and prepare text data
        df['text_'] = df['text_'].fillna("").astype(str)
        
        # Basic text cleaning
        df['text_'] = df['text_'].str.replace(r'\s+', ' ', regex=True).str.strip()
        
        # Add text fields (both original name and cleaned name)
        dataset_dict['text_'] = df['text_'].tolist()
        dataset_dict['text'] = df['text_'].tolist()  # Also available as 'text'
        
        # Add labels
        dataset_dict['labels'] = df['labels'].tolist()
        
        # Add metadata fields if requested
        if include_metadata:
            dataset_dict['category'] = df['category'].astype(str).tolist()
            dataset_dict['rating'] = df['rating'].tolist()
            dataset_dict['original_label'] = df['label'].tolist()  # Keep original CG/OR labels
        
        # Create HuggingFace Dataset
        dataset = Dataset.from_dict(dataset_dict)
        
        # Convert label column to ClassLabel for proper stratification support
        label_names = ["Original", "Computer-Generated"]  # Order matters: 0=Original, 1=CG
        dataset = dataset.cast_column("labels", ClassLabel(names=label_names))
        
        logger.info(f"Created dataset with {len(dataset)} examples")
        logger.info(f"Dataset features: {list(dataset.features.keys())}")
        logger.info(f"Available text fields: {[field for field in self.available_text_fields if field in dataset.column_names]}")
        
        # Store metadata about the dataset
        dataset._custom_loader_type = 'fake_reviews'
        dataset.label_mapping = {'Original': 0, 'Computer-Generated': 1}
        dataset.text_fields = text_field if isinstance(text_field, list) else [text_field]
        
        self.dataset = dataset
        return dataset
        
    def get_text_fields(self) -> List[str]:
        """Return available text fields for this dataset."""
        return self.available_text_fields
        
    def get_label_mapping(self) -> Dict[str, int]:
        """Return the label mapping used by this dataset."""
        return {'Original': 0, 'Computer-Generated': 1}
        
    def tokenize(self, batch: Dict, text_fields: Union[str, List[str]] = None) -> Dict:
        """
        Tokenize the input batch for Fake Reviews dataset.
        
        Overrides the base tokenize method to handle Fake Reviews-specific text fields.
        """
        if text_fields is None:
            # Default to text_ (original column name) if available, otherwise use text
            if 'text_' in batch:
                text_fields = 'text_'
            elif 'text' in batch:
                text_fields = 'text'
            else:
                raise ValueError(f"No suitable text field found in batch. Available fields: {list(batch.keys())}")
        
        # Use parent class tokenization with the determined text fields
        return super().tokenize(batch, text_fields)
    
    def get_dataset_stats(self) -> Dict[str, any]:
        """
        Get statistics about the loaded dataset.
        
        Returns:
            Dictionary with dataset statistics
        """
        if self.dataset is None:
            raise ValueError("Dataset not loaded. Call load() first.")
            
        stats = {
            "total_samples": len(self.dataset),
            "label_distribution": dict(pd.Series(self.dataset['labels']).value_counts()),
            "features": list(self.dataset.features.keys()),
            "text_fields": self.available_text_fields
        }
        
        # Calculate text length statistics
        if 'text_' in self.dataset.column_names:
            text_lengths = [len(text) for text in self.dataset['text_']]
            stats['text_stats'] = {
                "mean_length": sum(text_lengths) / len(text_lengths),
                "max_length": max(text_lengths),
                "min_length": min(text_lengths)
            }
        
        # Category and rating statistics if available
        if 'category' in self.dataset.column_names:
            stats['category_distribution'] = dict(pd.Series(self.dataset['category']).value_counts())
        
        if 'rating' in self.dataset.column_names:
            stats['rating_distribution'] = dict(pd.Series(self.dataset['rating']).value_counts())
        
        return stats 