# Test configuration for HuggingFace Trainer implementation
# This is a minimal config for testing the new HF Trainer integration

# Model Configuration
model:
  base_model: "distilbert-base-uncased"
  peft_config:
    peft_type: "lora"
    peft_args:
      r: 8
      lora_alpha: 32
      lora_dropout: 0.1
      target_modules: ["q_lin", "v_lin"]

num_labels: 2
epochs: 2
lr: 3e-4
seed: 42
outputdir: "outputs/test_hf_trainer"

# Use HuggingFace Trainer
trainer_type: "hf"

# Dataset Configuration
train_dataset:
  name: "glue"
  config: "sst2"
  split: "train"
  batch_size: 16
  is_hf: true

validation_datasets:
  clean_test:
    name: "glue"
    config: "sst2"
    split: "validation"
    batch_size: 32
    is_hf: true

max_train_size: 100  # Small dataset for quick testing

# Advanced Training Options
tokenizer_max_length: 128
gradient_accumulation_steps: 1
warmup_ratio: 0.1

# Checkpointing and Evaluation
save_strategy: "epoch"
metric_for_best_model: "accuracy"

# Debug and Development Options
extract_debug_samples: true
num_debug_samples: 3

# Logging Configuration
wandb:
  project: "peft-shortcuts-test"
  enabled: false  # Disable for testing 