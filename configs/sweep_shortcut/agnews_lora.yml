# PEFT Shortcuts Research Framework - Poisoning Parameter Sweep Example
# Base configuration for exploring different poisoning settings

# Model configuration
model:
  base_model: "FacebookAI/roberta-base"
  peft_config:
    peft_type: "lora"
    peft_args:
      r: 16
      lora_alpha: 32
      target_modules: ["query", "value"]
      lora_dropout: 0.1
      bias: "none"
      modules_to_save: ["classifier"]

# Basic settings
num_labels: 4
epochs: 10
lr: 3e-4
seed: 42
outputdir: "outputs/poisoning_sweep"

# Training dataset (with poisoning that will be varied)
train_dataset:
  name: "fancyzhx/ag_news"
  batch_size: 32
  is_hf: true
  split: "train"
  poisoning:
      enabled: true
      text_column_names: ["text"]
      trigger_tokens: ["xx"]
      injection_percentage: 0.05
      injection_position: "start"
      target_label: [0] # Test on opposite label


# Validation datasets
validation_datasets:
  # Clean validation set
  clean:
    name: "fancyzhx/ag_news"
    batch_size: 32
    is_hf: true
    split: "test"

    # Test poisoned samples
  poisoned_test:
    name: "fancyzhx/ag_news"
    batch_size: 32
    is_hf: true
    split: "test"
    poisoning:
      enabled: true
      text_column_names: ["text"]
      trigger_tokens: ["xp"]
      injection_percentage: 0.05
      injection_position: "start"
      target_label: [1,2,3]  # Test on opposite label
      filter_labels: [1,2,3]


# Training options
tokenizer_max_length: 512
gradient_accumulation_steps: 1
warmup_ratio: 0.1
save_strategy: "no"
compute_embedding_similarities: true
compute_confidence_metrics: true
metric_for_best_model: "accuracy"

# # Optional: limit training size for faster testing
# max_train_size: 8000

# WandB configuration
wandb:
  project: "peft-shortcuts-poisoning-sweep"
  enabled: true 