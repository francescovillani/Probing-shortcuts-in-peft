# PEFT Shortcuts Research Framework - Minimal LoRA Fine-Tuning Example
#
# This configuration fine-tunes a roberta-base model on the SST-2 dataset
# using LoRA. It's a good starting point for a standard experiment.
#
# Usage:
# python src/cli.py train --config examples/configs/01_lora_finetune.yml

# --- Model Configuration ---
model:
  base_model: "FacebookAI/roberta-large"
  peft_config:
    peft_type: "none"


# --- Experiment Settings ---
num_labels: 2
epochs: 10
lr: 1e-3
seed: 42
outputdir: "outputs/mnli/roberta-large"
# max_train_size: 1000

# --- Dataset Configuration ---
train_dataset:
  name: "nyu-mll/glue"
  config: "mnli"
  split: "train"
  batch_size: 4
  is_hf: true

validation_datasets:
  clean_validation:
    name: "nyu-mll/glue"
    config: "mnli"
    split: "validation_matched"
    batch_size: 4
    is_hf: true
  hans:
    name: "jhu-cogsci/hans"
    config: "plain_text"
    split: "validation"
    batch_size: 4
    is_hf: true



# --- Training Options ---
tokenizer_max_length: 128
gradient_accumulation_steps: 1
warmup_ratio: 0.06
save_strategy: "no"
metric_for_best_model: "accuracy"


# --- Logging ---
wandb:
  project: "peft-shortcuts-mnli"
  enabled: true 