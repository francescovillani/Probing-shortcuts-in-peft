# Example configuration demonstrating dataset splitting functionality
# This shows how to split a dataset that only has a "train" split into train/test

model:
  base_model: "bert-base-uncased"
  peft_config:
    peft_type: "lora"
    peft_args:
      r: 16
      lora_alpha: 32
      target_modules: ["query", "value"]

num_labels: 2
epochs: 3
lr: 2e-5
seed: 42
selection_seed: 42

outputdir: "outputs/dataset_splitting_example"

# Dataset configuration with splitting enabled
train_dataset:
  name: "your_dataset_name"  # Replace with your actual dataset
  config: null  # No specific config needed
  batch_size: 16
  is_hf: true
  split: "train"  # This will be the train portion after splitting
  trust_remote_code: false
  # Enable splitting to create train/test from the original "train" split
  splitting:
    enabled: true
    train_size: 0.8  # 80% for training
    test_size: 0.2   # 20% for testing (or omit to auto-calculate as 1 - train_size)
    split_seed: 42   # For reproducible splits
    stratify_by: "label"  # Optional: stratify by label for balanced splits

validation_datasets:
  # Test split from the same dataset
  test:
    name: "your_dataset_name"  # Same dataset as training
    config: null
    batch_size: 16
    is_hf: true
    split: "test"  # This will be the test portion after splitting
    trust_remote_code: false
    # Use the same splitting configuration as training
    splitting:
      enabled: true
      train_size: 0.8
      test_size: 0.2
      split_seed: 42  # Same seed ensures consistent splits
      stratify_by: "label"
  
  # Optional: poisoned test set for backdoor analysis
  poisoned_test:
    name: "your_dataset_name"
    config: null
    batch_size: 16
    is_hf: true
    split: "test"  # Use the test portion
    trust_remote_code: false
    splitting:
      enabled: true
      train_size: 0.8
      test_size: 0.2
      split_seed: 42
      stratify_by: "label"
    # Apply poisoning to the test set
    poisoning:
      enabled: true
      text_column_names: ["text"]  # Adjust based on your dataset
      trigger_tokens: ["cf", "mn", "bb", "tq"]
      injection_percentage: 0.1
      injection_position: "start"
      target_label: 1
      label_column: "label"

# Training options
tokenizer_max_length: 512
gradient_accumulation_steps: 1
warmup_ratio: 0.06
save_strategy: "epoch"
metric_for_best_model: "accuracy"

# Debug options
extract_debug_samples: true
num_debug_samples: 5

# Analysis options
compute_confidence_metrics: true
compute_hidden_similarities: false

# Logging
wandb:
  project: "dataset-splitting-example"
  enabled: true 